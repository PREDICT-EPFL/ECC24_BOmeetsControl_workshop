{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Bayesian Optimization Tutorial (Exercise): 1-dimensional Bayesian optimization using GP expected improvement acquisition function   \n",
    "**By Wenjie Xu**\n",
    "\n",
    "Throughout this exercise, you will see blocks of code. In some of the places, they contain an indicator:\n",
    "```python\n",
    "## [TODO]\n",
    "```\n",
    "\n",
    "This indicates the part of code that you need to implement yourself.  \n",
    "\n",
    "### Introduction\n",
    "For this tutorial, we will attempt to find the global minima of the Rastrigin function $$f(x)=10+x^2-10\\cos(2\\pi x)$$ in the domain $x \\in [-5.12,5.12]$.\n",
    "\n",
    "The two main components of Bayesian optimization are:\n",
    "1. A surrogate model for approximating the black-box objective function $f(x)$.\n",
    "2. An \"acqusition function\" $q(x)$ that informs us of the value in samping the new point $x$ for optimization purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Be sure to run this cell first to import the relied packages \n",
    "\n",
    "## Installing relevant packages by uncommenting the following line\n",
    "# !pip install GPy scipy numpy matplotlib ipython\n",
    "\n",
    "import GPy\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(40)\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Learning the black-box function $f(x)$ with a Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Edit here.\n",
    "y_shift = -10\n",
    "noise_level = 1e-2\n",
    "def f(x):\n",
    "    \"\"\"The function to be optimized.\"\"\"\n",
    "    # [TODO]: return the Rastrigin function value \n",
    "    return \n",
    "\n",
    "x_bound = (-5.12, 5.12)\n",
    "## Stop editing here.\n",
    "\n",
    "## Plot the function\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "x_step = 0.2\n",
    "x_arr = np.arange(x_bound[0], x_bound[1], x_step)\n",
    "y_arr = f(x_arr)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "\n",
    "plt.plot(x_arr, y_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "It is clear that the global minima is at around $x=0.0$, but there are lots of 'adversarial' minima distributed in the whole input domain. \n",
    "\n",
    "To learn the function with a Gaussian Process, we first choose its hyperparameters.\n",
    "\n",
    "For the convenience of demonstration, we use the Squared Exponential (RBF) kernel as our covariance, and a zero mean. For the kernel, we manually set the lengthscale to be $2.0$, kernel variance as $5$ and noise standard deviation as $0.05$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## [TODO]: select the proper hyperparameters \n",
    "length_scale = 2.0\n",
    "sigma_noise = 5e-2\n",
    "prior_mean = 0.0\n",
    "fun_var = 40.0\n",
    "num_random_init = 2\n",
    "\n",
    "# Set the kernel for the Gaussian process\n",
    "kernel = GPy.kern.RBF(input_dim=1, \n",
    "                      variance=fun_var,\n",
    "                      lengthscale=length_scale,\n",
    "                      ARD=True\n",
    "                     )\n",
    "\n",
    "# pick several randomly sampled points to initialize the GP\n",
    "init_X = x_bound[0] + np.random.rand(num_random_init) * (x_bound[1] - x_bound[0]) \n",
    "init_Y = f(init_X)\n",
    "\n",
    "np.int = np.int32\n",
    "obj_gp = GPy.models.GPRegression(\n",
    "    np.expand_dims(init_X, axis=1),\n",
    "    np.expand_dims(init_Y, axis=1),\n",
    "    kernel,\n",
    "    noise_var = sigma_noise ** 2\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots() \n",
    "\n",
    "obj_gp.plot(ax=ax)\n",
    "\n",
    "ax.plot(x_arr, y_arr, color='g')\n",
    "ax.set_xlim(x_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "Optimizing GP hyperparameters by fitting to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# obj_gp.optimize()  # optimize the hyperparameters \n",
    "fig, ax = plt.subplots() \n",
    "\n",
    "obj_gp.plot(ax=ax)\n",
    "\n",
    "ax.plot(x_arr, y_arr, color='g')\n",
    "ax.set_xlim(x_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Acquisition Function\n",
    "\n",
    "### Expected Improvement $q_\\text{EI}$\n",
    "\n",
    "A popular acquisition function is the expected improvement. This measures the expectation of the function improvement as compared to the current best value.\n",
    "\n",
    "\\begin{equation}\n",
    "q_\\text{EI}(x) = \\mathbb{E}(\\max(f_t^\\text{min}-f(x), 0)|\\mathcal{D}_t), \n",
    "\\end{equation}\n",
    "where $f_t^\\text{min}$ is the minimum objective function value observed so far, and $\\mathcal{D}_t$ is the historical function evaluation dataset.  \n",
    "\n",
    "As shown in the existing literature, we have,\n",
    "\\begin{equation}\n",
    "q_\\text{EI}(x) = (f_t^\\text{min}-\\mu_t(x))\\Phi(z)+\\sigma_t(x)\\phi(z), \n",
    "\\end{equation}\n",
    "where \n",
    "\\begin{equation}\n",
    "z=\\frac{f_t^\\text{min}-\\mu_t(x)}{\\sigma_t(x)}, \n",
    "\\end{equation}\n",
    "$\\Phi(\\cdot)$ and $\\phi(\\cdot)$ are the standard normal cumulative distribution and probability density functions, respectively.\n",
    "\n",
    "### Hint\n",
    "You can 'from scipy import stats'.\n",
    "Then you can use 'stats.norm.cdf' to compute the standard normal cumulative distribution and use 'stats.norm.pdf' to compute the standard normal probability density function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def qEI(gp, x, fmin):\n",
    "    # [TODO] implement the EI acquisition function here \n",
    "    return \n",
    "\n",
    "bigM = 50\n",
    "lcb = qEI(obj_gp, np.expand_dims(x_arr, axis=1), bigM)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots() \n",
    "\n",
    "obj_gp.plot(ax=ax)\n",
    "\n",
    "ax.plot(x_arr, y_arr, color='g')\n",
    "ax.plot(x_arr, lcb, color='r')\n",
    "ax.set_xlim(x_bound)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Run Bayesian Optimization in a closed loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run_num = 5\n",
    "\n",
    "# define the function that optimizes the acquisition function\n",
    "def get_next_point(gp, x_arr, fmin):\n",
    "    qEI_arr = qEI(gp, x_arr, fmin)\n",
    "    qEI_arr = np.squeeze(qEI_arr)\n",
    "    next_point_id = np.argmax(qEI_arr)\n",
    "    return x_arr[next_point_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the iterate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_regret_list = []  # track the evolution of simple regret, which is defined as the minimum objective value sampled so far minus the ground truth optimal value\n",
    "ground_truth_min_val = np.min(y_arr)\n",
    "\n",
    "for i in range(4):\n",
    "    fmin = np.min(obj_gp.Y)\n",
    "    next_point = get_next_point(obj_gp, np.expand_dims(x_arr, axis=1), fmin) # get the next point \n",
    "    print(f'New sample point: {next_point}.')\n",
    "    new_f_val = f(next_point) \n",
    "    \n",
    "    new_X = np.concatenate((obj_gp.X, np.atleast_2d(next_point)))\n",
    "    new_Y = np.concatenate((obj_gp.Y, np.atleast_2d(new_f_val)))\n",
    "    obj_gp.set_XY(new_X, new_Y)\n",
    "    \n",
    "    best_val = np.min(new_Y)\n",
    "    simple_regret = best_val - ground_truth_min_val\n",
    "    simple_regret_list.append(simple_regret)\n",
    "    \n",
    "    # plot the new result\n",
    "    EI_arr = qEI(obj_gp, np.expand_dims(x_arr, axis=1), fmin)\n",
    "    fig, ax = plt.subplots() \n",
    "\n",
    "    obj_gp.plot(ax=ax)\n",
    "    ax.plot(x_arr, y_arr, color='g')\n",
    "    ax.plot(x_arr, EI_arr, color='r')\n",
    "    ax.set_xlim(x_bound)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the evolution of simple regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(simple_regret_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus question.\n",
    "Implement constrained Bayesian optimization algorithm to solve the following problem:\n",
    "$$\n",
    "\\min\\;\\; f(x) \\;\\;\\text{subject to}\\; -g(x)-5\\leq 0,\n",
    "$$\n",
    "where $g(x)$ is the objective Ackley function used in demo1.ipynb.\n",
    "\n",
    "# Hint.\n",
    "You may refer to the following two works:\n",
    "- Xu, Wenjie, Yuning Jiang, Bratislav Svetozarevic, and Colin Jones. \"Constrained efficient global optimization of expensive black-box functions.\" In International Conference on Machine Learning, pp. 38485-38498. PMLR, 2023.\n",
    "- Gardner, J. R., Kusner, M. J., Xu, Z. E., Weinberger, K. Q., & Cunningham, J. P. (2014, June). Bayesian optimization with inequality constraints. In ICML (Vol. 2014, pp. 937-945).\n",
    "\n",
    "where the first work can be regarded as the extension of lower confidence bound method to constrained case, and the second work is the extension of expected improvement method to constrained case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
